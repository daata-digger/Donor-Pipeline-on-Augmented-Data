{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26f77850",
   "metadata": {},
   "source": [
    "# Donor Analytics Workflow - Comprehensive Analysis\n",
    "\n",
    "This notebook provides a complete workflow for analyzing donor data, including:\n",
    "1. Data Quality Assessment\n",
    "2. Geospatial Analysis\n",
    "3. Advanced Analytics\n",
    "4. Machine Learning Pipeline\n",
    "5. Visualization Functions\n",
    "6. Cloud Configuration\n",
    "7. Streamlit Dashboard Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74115863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import xgboost as xgb\n",
    "from datetime import datetime, timedelta\n",
    "import great_expectations as ge\n",
    "import streamlit as st\n",
    "from pathlib import Path\n",
    "\n",
    "# Set plotting styles\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4e3a9b",
   "metadata": {},
   "source": [
    "# 1. Data Quality Assessment\n",
    "\n",
    "First, we'll load the data and perform comprehensive quality checks using pandas-profiling and Great Expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b621e0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data_dir = Path(\"../data\")\n",
    "donors = pd.read_csv(data_dir / \"raw/donors.csv\")\n",
    "donations = pd.read_csv(data_dir / \"raw/donations.csv\")\n",
    "campaigns = pd.read_csv(data_dir / \"raw/campaigns.csv\")\n",
    "events = pd.read_csv(data_dir / \"raw/engagement_events.csv\")\n",
    "wealth = pd.read_csv(data_dir / \"raw/wealth_external.csv\")\n",
    "\n",
    "# Create Great Expectations DataContext\n",
    "context = ge.data_context.DataContext()\n",
    "\n",
    "# Create expectations for donors dataset\n",
    "donor_suite = ge.dataset.PandasDataset(donors)\n",
    "donor_suite.expect_column_values_to_not_be_null(\"donor_id\")\n",
    "donor_suite.expect_column_values_to_be_unique(\"donor_id\")\n",
    "donor_suite.expect_column_values_to_be_between(\"birth_year\", 1900, datetime.now().year)\n",
    "donor_suite.expect_column_values_to_match_regex(\"email\", r\"[^@]+@[^@]+\\.[^@]+\")\n",
    "donor_suite.expect_column_values_to_be_in_set(\"state\", ['NY', 'NJ', 'CT', 'PA', 'MA'])  # Adjust states as needed\n",
    "\n",
    "# Generate validation report\n",
    "validation_result = donor_suite.validate()\n",
    "print(f\"Data Quality Score: {validation_result.success_percent:.2f}%\")\n",
    "\n",
    "# Display detailed issues\n",
    "issues = []\n",
    "for result in validation_result.results:\n",
    "    if not result.success:\n",
    "        issues.append({\n",
    "            'expectation': result.expectation_config.expectation_type,\n",
    "            'column': result.expectation_config.kwargs.get('column'),\n",
    "            'success_rate': result.success_percent\n",
    "        })\n",
    "pd.DataFrame(issues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f10d21",
   "metadata": {},
   "source": [
    "# 2. Geospatial Analysis\n",
    "\n",
    "Now we'll analyze the geographical distribution of donors and donations using GeoPandas and Folium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b487711c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load US states shapefile\n",
    "states = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "states = states[states['continent'] == 'North America']\n",
    "\n",
    "# Aggregate donor data by state\n",
    "state_stats = donors.groupby('state').agg({\n",
    "    'donor_id': 'count',\n",
    "    'wealth_index': 'mean',\n",
    "    'engagement_index': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Create choropleth map\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Choropleth(\n",
    "    locations=state_stats['state'],\n",
    "    z=state_stats['donor_id'],\n",
    "    locationmode='USA-states',\n",
    "    colorscale='Viridis',\n",
    "    colorbar_title=\"Number of Donors\"\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Donor Distribution by State',\n",
    "    geo_scope='usa',\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Create interactive Folium map\n",
    "donor_map = folium.Map(location=[40.7128, -74.0060], zoom_start=4)\n",
    "\n",
    "# Add donor clusters\n",
    "for idx, row in donors.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        radius=5,\n",
    "        popup=f\"Donor ID: {row['donor_id']}<br>Wealth Index: {row['wealth_index']:.2f}\",\n",
    "        color='red',\n",
    "        fill=True\n",
    "    ).add_to(donor_map)\n",
    "\n",
    "donor_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad07daf",
   "metadata": {},
   "source": [
    "# 3. Advanced Analytics\n",
    "\n",
    "Let's perform advanced statistical analysis and feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f02de30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RFM scores\n",
    "def calculate_rfm(donations_df, analysis_date=None):\n",
    "    if analysis_date is None:\n",
    "        analysis_date = datetime.now()\n",
    "    \n",
    "    rfm = donations.groupby('donor_id').agg({\n",
    "        'donation_date': lambda x: (analysis_date - pd.to_datetime(x.max())).days,  # Recency\n",
    "        'donation_id': 'count',  # Frequency\n",
    "        'amount': 'sum'  # Monetary\n",
    "    }).reset_index()\n",
    "    \n",
    "    rfm.columns = ['donor_id', 'recency_days', 'frequency', 'monetary']\n",
    "    \n",
    "    # Score from 1-5 (5 being best)\n",
    "    rfm['r_score'] = pd.qcut(rfm['recency_days'], q=5, labels=[5,4,3,2,1])\n",
    "    rfm['f_score'] = pd.qcut(rfm['frequency'], q=5, labels=[1,2,3,4,5])\n",
    "    rfm['m_score'] = pd.qcut(rfm['monetary'], q=5, labels=[1,2,3,4,5])\n",
    "    \n",
    "    rfm['rfm_score'] = rfm['r_score'].astype(str) + rfm['f_score'].astype(str) + rfm['m_score'].astype(str)\n",
    "    \n",
    "    return rfm\n",
    "\n",
    "# Calculate RFM scores\n",
    "rfm = calculate_rfm(donations)\n",
    "\n",
    "# Visualize RFM segments\n",
    "fig = make_subplots(rows=2, cols=2)\n",
    "\n",
    "# RFM Score Distribution\n",
    "rfm_dist = rfm['rfm_score'].value_counts()[:10]\n",
    "fig.add_trace(\n",
    "    go.Bar(x=rfm_dist.index, y=rfm_dist.values, name='RFM Segments'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Monetary vs Frequency\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=rfm['frequency'], y=rfm['monetary'], mode='markers',\n",
    "               marker=dict(color=rfm['recency_days'], colorscale='Viridis'),\n",
    "               name='M vs F'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Recency Distribution\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=rfm['recency_days'], name='Recency Distribution'),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Average Amount by Frequency\n",
    "avg_by_freq = donations.groupby('donor_id').agg({\n",
    "    'donation_id': 'count',\n",
    "    'amount': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Box(y=avg_by_freq['amount'], x=pd.qcut(avg_by_freq['donation_id'], 5).astype(str),\n",
    "           name='Amount by Frequency Quintile'),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=800, title_text=\"RFM Analysis Dashboard\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42eca34",
   "metadata": {},
   "source": [
    "# 4. Machine Learning Pipeline\n",
    "\n",
    "Now we'll create a machine learning pipeline for donor propensity scoring and clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9188a715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for ML\n",
    "def prepare_ml_features(donors_df, rfm_df, events_df, wealth_df):\n",
    "    features = donors_df.merge(rfm_df[['donor_id', 'recency_days', 'frequency', 'monetary']], \n",
    "                             on='donor_id', how='left')\n",
    "    features = features.merge(events_df, on='donor_id', how='left')\n",
    "    features = features.merge(wealth_df, on='donor_id', how='left')\n",
    "    \n",
    "    # Fill missing values\n",
    "    features = features.fillna({\n",
    "        'events_attended': 0,\n",
    "        'volunteer_hours': 0,\n",
    "        'wealth_score_ext': features['wealth_score_ext'].median()\n",
    "    })\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Prepare data\n",
    "features_df = prepare_ml_features(donors, rfm, events, wealth)\n",
    "\n",
    "# Define features for clustering\n",
    "cluster_features = [\n",
    "    'monetary', 'frequency', 'recency_days',\n",
    "    'events_attended', 'volunteer_hours', 'wealth_score_ext'\n",
    "]\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(features_df[cluster_features])\n",
    "\n",
    "# Perform k-means clustering\n",
    "n_clusters = 5\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "features_df['cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Analyze clusters\n",
    "cluster_summary = features_df.groupby('cluster').agg({\n",
    "    'monetary': 'mean',\n",
    "    'frequency': 'mean',\n",
    "    'recency_days': 'mean',\n",
    "    'events_attended': 'mean',\n",
    "    'volunteer_hours': 'mean',\n",
    "    'wealth_score_ext': 'mean',\n",
    "    'donor_id': 'count'\n",
    "}).round(2)\n",
    "\n",
    "# Visualize clusters\n",
    "fig = px.scatter_3d(\n",
    "    features_df,\n",
    "    x='monetary',\n",
    "    y='frequency',\n",
    "    z='recency_days',\n",
    "    color='cluster',\n",
    "    size='wealth_score_ext',\n",
    "    hover_data=['donor_id', 'events_attended'],\n",
    "    title='Donor Clusters (3D View)'\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Train XGBoost model for propensity scoring\n",
    "def create_target(row):\n",
    "    \"\"\"Define target variable based on RFM values\"\"\"\n",
    "    if (row['recency_days'] < 120 or \n",
    "        row['frequency'] >= 3 or \n",
    "        row['monetary'] >= features_df['monetary'].median()):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "features_df['target'] = features_df.apply(create_target, axis=1)\n",
    "\n",
    "# Prepare features for XGBoost\n",
    "X = features_df[cluster_features]\n",
    "y = features_df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = xgb.XGBClassifier(\n",
    "    max_depth=4,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Feature importance plot\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': cluster_features,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "px.bar(\n",
    "    feature_importance,\n",
    "    x='feature',\n",
    "    y='importance',\n",
    "    title='Feature Importance in Propensity Model'\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f746740",
   "metadata": {},
   "source": [
    "# 5. Cloud Configuration\n",
    "\n",
    "Set up configuration templates for different cloud providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956ab460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud configuration templates\n",
    "cloud_configs = {\n",
    "    'aws': {\n",
    "        'storage': {\n",
    "            'bucket': 'donor-analytics-data',\n",
    "            'region': 'us-east-1',\n",
    "            'access_key': '${AWS_ACCESS_KEY}',\n",
    "            'secret_key': '${AWS_SECRET_KEY}'\n",
    "        },\n",
    "        'compute': {\n",
    "            'emr_cluster': {\n",
    "                'name': 'donor-analytics-cluster',\n",
    "                'instance_type': 'm5.xlarge',\n",
    "                'instance_count': 3,\n",
    "                'applications': ['Spark', 'Hadoop']\n",
    "            }\n",
    "        },\n",
    "        'warehouse': {\n",
    "            'snowflake': {\n",
    "                'account': '${SNOWFLAKE_ACCOUNT}',\n",
    "                'username': '${SNOWFLAKE_USER}',\n",
    "                'password': '${SNOWFLAKE_PASSWORD}',\n",
    "                'warehouse': 'DONOR_ANALYTICS_WH',\n",
    "                'database': 'DONOR_ANALYTICS_DB'\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'azure': {\n",
    "        'storage': {\n",
    "            'account_name': 'donoranalytics',\n",
    "            'container': 'donor-data',\n",
    "            'connection_string': '${AZURE_STORAGE_CONNECTION_STRING}'\n",
    "        },\n",
    "        'compute': {\n",
    "            'synapse': {\n",
    "                'workspace_name': 'donor-analytics-synapse',\n",
    "                'spark_pool': 'DonorAnalyticsPool',\n",
    "                'node_size': 'Medium',\n",
    "                'node_count': 3\n",
    "            }\n",
    "        },\n",
    "        'warehouse': {\n",
    "            'synapse_sql': {\n",
    "                'database': 'DonorAnalyticsDB',\n",
    "                'server': '${AZURE_SQL_SERVER}',\n",
    "                'user': '${AZURE_SQL_USER}',\n",
    "                'password': '${AZURE_SQL_PASSWORD}'\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save configurations\n",
    "import yaml\n",
    "import json\n",
    "\n",
    "# Save as YAML\n",
    "with open('../config/cloud_config.yml', 'w') as f:\n",
    "    yaml.dump(cloud_configs, f)\n",
    "\n",
    "# Save as JSON\n",
    "with open('../config/cloud_config.json', 'w') as f:\n",
    "    json.dump(cloud_configs, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35211ac8",
   "metadata": {},
   "source": [
    "# 6. Streamlit Dashboard Components\n",
    "\n",
    "Create reusable components for the Streamlit dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72792cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Streamlit dashboard components\n",
    "class DonorDashboard:\n",
    "    def __init__(self, features_df, model):\n",
    "        self.features_df = features_df\n",
    "        self.model = model\n",
    "        \n",
    "    def donor_search(self):\n",
    "        \"\"\"Create donor search component\"\"\"\n",
    "        st.sidebar.header(\"ðŸ” Donor Search\")\n",
    "        search_term = st.sidebar.text_input(\"Search by name or ID\")\n",
    "        \n",
    "        if search_term:\n",
    "            results = self.features_df[\n",
    "                self.features_df['donor_id'].astype(str).str.contains(search_term) |\n",
    "                self.features_df['first_name'].str.contains(search_term, case=False) |\n",
    "                self.features_df['last_name'].str.contains(search_term, case=False)\n",
    "            ]\n",
    "            return results\n",
    "        return None\n",
    "    \n",
    "    def donor_profile(self, donor_id):\n",
    "        \"\"\"Create donor profile component\"\"\"\n",
    "        donor = self.features_df[self.features_df['donor_id'] == donor_id].iloc[0]\n",
    "        \n",
    "        st.header(f\"Donor Profile: {donor['first_name']} {donor['last_name']}\")\n",
    "        \n",
    "        col1, col2, col3 = st.columns(3)\n",
    "        \n",
    "        with col1:\n",
    "            st.metric(\"Total Giving\", f\"${donor['monetary']:,.2f}\")\n",
    "        with col2:\n",
    "            st.metric(\"Frequency\", f\"{donor['frequency']} gifts\")\n",
    "        with col3:\n",
    "            st.metric(\"Last Gift\", f\"{donor['recency_days']} days ago\")\n",
    "            \n",
    "        # Propensity score\n",
    "        features = donor[self.model.feature_names_in_]\n",
    "        propensity = self.model.predict_proba([features])[0][1]\n",
    "        \n",
    "        st.progress(propensity)\n",
    "        st.caption(f\"Propensity to Give: {propensity:.1%}\")\n",
    "        \n",
    "        return donor\n",
    "    \n",
    "    def geographic_view(self):\n",
    "        \"\"\"Create geographic visualization component\"\"\"\n",
    "        st.header(\"ðŸ“ Geographic Distribution\")\n",
    "        \n",
    "        # Create choropleth\n",
    "        state_summary = self.features_df.groupby('state').agg({\n",
    "            'monetary': 'sum',\n",
    "            'donor_id': 'count'\n",
    "        }).reset_index()\n",
    "        \n",
    "        fig = go.Figure(data=go.Choropleth(\n",
    "            locations=state_summary['state'],\n",
    "            z=state_summary['monetary'],\n",
    "            locationmode='USA-states',\n",
    "            colorscale='Viridis',\n",
    "            colorbar_title=\"Total Donations ($)\"\n",
    "        ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title='Donation Distribution by State',\n",
    "            geo_scope='usa',\n",
    "        )\n",
    "        \n",
    "        st.plotly_chart(fig)\n",
    "    \n",
    "    def rfm_analysis(self):\n",
    "        \"\"\"Create RFM analysis component\"\"\"\n",
    "        st.header(\"ðŸ“Š RFM Analysis\")\n",
    "        \n",
    "        # RFM Distribution\n",
    "        fig = make_subplots(rows=1, cols=3)\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Histogram(x=self.features_df['recency_days'], name=\"Recency\"),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Histogram(x=self.features_df['frequency'], name=\"Frequency\"),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Histogram(x=self.features_df['monetary'], name=\"Monetary\"),\n",
    "            row=1, col=3\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(height=400, title_text=\"RFM Distributions\")\n",
    "        st.plotly_chart(fig)\n",
    "    \n",
    "    def campaign_simulator(self):\n",
    "        \"\"\"Create campaign simulator component\"\"\"\n",
    "        st.header(\"ðŸŽ¯ Campaign Simulator\")\n",
    "        \n",
    "        col1, col2 = st.columns(2)\n",
    "        \n",
    "        with col1:\n",
    "            min_propensity = st.slider(\n",
    "                \"Minimum Propensity Score\",\n",
    "                min_value=0.0,\n",
    "                max_value=1.0,\n",
    "                value=0.5\n",
    "            )\n",
    "        \n",
    "        with col2:\n",
    "            min_amount = st.slider(\n",
    "                \"Minimum Previous Gift\",\n",
    "                min_value=0,\n",
    "                max_value=int(self.features_df['monetary'].max()),\n",
    "                value=1000\n",
    "            )\n",
    "        \n",
    "        # Filter prospects\n",
    "        prospects = self.features_df[\n",
    "            (self.model.predict_proba(self.features_df[self.model.feature_names_in_])[:, 1] >= min_propensity) &\n",
    "            (self.features_df['monetary'] >= min_amount)\n",
    "        ]\n",
    "        \n",
    "        st.metric(\"Target List Size\", len(prospects))\n",
    "        st.metric(\"Potential Revenue\", f\"${prospects['monetary'].sum():,.2f}\")\n",
    "        \n",
    "        return prospects\n",
    "\n",
    "# Example usage (this won't run in notebook, but shows how to use in Streamlit)\n",
    "\"\"\"\n",
    "if __name__ == \"__main__\":\n",
    "    st.set_page_config(\n",
    "        page_title=\"Donor Analytics Dashboard\",\n",
    "        page_icon=\"ðŸŽ¯\",\n",
    "        layout=\"wide\"\n",
    "    )\n",
    "    \n",
    "    # Initialize dashboard\n",
    "    dashboard = DonorDashboard(features_df, model)\n",
    "    \n",
    "    # Sidebar navigation\n",
    "    page = st.sidebar.selectbox(\n",
    "        \"Navigate\",\n",
    "        [\"Donor Search\", \"Geographic View\", \"RFM Analysis\", \"Campaign Simulator\"]\n",
    "    )\n",
    "    \n",
    "    if page == \"Donor Search\":\n",
    "        results = dashboard.donor_search()\n",
    "        if results is not None:\n",
    "            selected_donor = st.selectbox(\n",
    "                \"Select Donor\",\n",
    "                results['donor_id'].tolist()\n",
    "            )\n",
    "            if selected_donor:\n",
    "                dashboard.donor_profile(selected_donor)\n",
    "                \n",
    "    elif page == \"Geographic View\":\n",
    "        dashboard.geographic_view()\n",
    "        \n",
    "    elif page == \"RFM Analysis\":\n",
    "        dashboard.rfm_analysis()\n",
    "        \n",
    "    elif page == \"Campaign Simulator\":\n",
    "        prospects = dashboard.campaign_simulator()\n",
    "        if len(prospects) > 0:\n",
    "            st.dataframe(prospects)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
